{
 "cells": [
  {
   "source": [
    "# Building the Civic Issue Classifier\n",
    "\n",
    "https://medium.com/@saad.arshad102/sentiment-analysis-text-classification-using-rnn-bi-lstm-recurrent-neural-network-81086dda8472\n",
    "https://www.tensorflow.org/tutorials/text/text_classification_rnn"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "!pip install gensim"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Adithi\n[nltk_data]     Satish\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "32120"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "temp = pd.read_csv('finalData.csv')\n",
    "data = pd.DataFrame()\n",
    "data = temp[['description', 'civic_issue']].copy()\n",
    "data.head()\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                         description  civic_issue\n",
       "0                          Garbage behind the temple            1\n",
       "1                                      Air pollution            1\n",
       "2                            Air pollution in hebbal            1\n",
       "3                         Garbage is dumped near BES            1\n",
       "4  Leaf and garbage burning on Shakthi Ganapathi ...            1"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>description</th>\n      <th>civic_issue</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Garbage behind the temple</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Air pollution</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Air pollution in hebbal</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Garbage is dumped near BES</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Leaf and garbage burning on Shakthi Ganapathi ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "#number of rows, columns\n",
    "data.shape \n",
    "\n",
    "#number of positive, negative (positive -> civic, negative -> non civic)\n",
    "data.civic_issue.value_counts()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0                                Garbage behind the temple\n1                                            Air pollution\n2                                  Air pollution in hebbal\n3                               Garbage is dumped near BES\n4        Leaf and garbage burning on Shakthi Ganapathi ...\n                               ...                        \n32115    gets new phone hopefully tomorrow but mail doe...\n32116    LeslieG stack is injured  are you a rowdy fan ...\n32117                        ryanbader  Bummerlove Chuck  \n32118         Kinda sad that my show time is over for now \n32119              good night and its still only day four \nName: Text_Clean, Length: 32120, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#data cleaning\n",
    "# 1) we remove punctuation marks\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    text_nopunct = ''\n",
    "    text_nopunct = re.sub('['+string.punctuation+']', '', text)\n",
    "    #text_nopunct = str.maketrans('', '', string.punctuation)\n",
    "    return text_nopunct\n",
    "data['Text_Clean'] = data['description'].apply(lambda x: remove_punctuation(x))\n",
    "\n",
    "print(data['Text_Clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) tokenisation (split the sentences into its respective words)\n",
    "tokens = [(word_tokenize(str(sen))) for sen in data.Text_Clean]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3) convert these tokenised words into their lower cases\n",
    "def lower_token(tokens): \n",
    "    return [w.lower() for w in tokens]    \n",
    "    \n",
    "lower_tokens = [lower_token(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) remove the stopwords using NLTK's stopwords\n",
    "stoplist = stopwords.words('english')\n",
    "def removeStopWords(tokens): \n",
    "    return [word for word in tokens if word not in stoplist]\n",
    "filtered_words = [removeStopWords(sen) for sen in lower_tokens]\n",
    "data['Text_Final'] = [' '.join(sen) for sen in filtered_words]\n",
    "data['tokens'] = filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                          Text_Final  \\\n",
       "0                              garbage behind temple   \n",
       "1                                      air pollution   \n",
       "2                               air pollution hebbal   \n",
       "3                            garbage dumped near bes   \n",
       "4  leaf garbage burning shakthi ganapathi temple ...   \n",
       "\n",
       "                                              tokens  civic_issue  Civic  \\\n",
       "0                          [garbage, behind, temple]            1      1   \n",
       "1                                   [air, pollution]            1      1   \n",
       "2                           [air, pollution, hebbal]            1      1   \n",
       "3                       [garbage, dumped, near, bes]            1      1   \n",
       "4  [leaf, garbage, burning, shakthi, ganapathi, t...            1      1   \n",
       "\n",
       "   Non_Civic  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text_Final</th>\n      <th>tokens</th>\n      <th>civic_issue</th>\n      <th>Civic</th>\n      <th>Non_Civic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>garbage behind temple</td>\n      <td>[garbage, behind, temple]</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>air pollution</td>\n      <td>[air, pollution]</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>air pollution hebbal</td>\n      <td>[air, pollution, hebbal]</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>garbage dumped near bes</td>\n      <td>[garbage, dumped, near, bes]</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>leaf garbage burning shakthi ganapathi temple ...</td>\n      <td>[leaf, garbage, burning, shakthi, ganapathi, t...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "# since we have a binary category model (civic issue or non civic issue), \n",
    "#we add two one hot encoded columns to our data frame\n",
    "\n",
    "civic = []\n",
    "non_civic = []\n",
    "for l in data.civic_issue:\n",
    "    if l == 0:\n",
    "        civic.append(0)\n",
    "        non_civic.append(1)\n",
    "    elif l == 1:\n",
    "        civic.append(1)\n",
    "        non_civic.append(0)\n",
    "data['Civic']= civic\n",
    "data['Non_Civic']= non_civic\n",
    "data = data[['Text_Final', 'tokens', 'civic_issue', 'Civic', 'Non_Civic']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we split the data into training and testing. i have taken 80-20. \n",
    "# not entirely sure if we should use the 'random_state' parameter.\n",
    "\n",
    "data_train, data_test = train_test_split(data, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "263691 words total, with a vocabulary size of 34562\nMax sentence length is 47\n"
     ]
    }
   ],
   "source": [
    "# we find the total number of words in our training dataset and find max sentence lengeth.\n",
    "\n",
    "all_training_words = [word for tokens in data_train[\"tokens\"] for word in tokens]\n",
    "training_sentence_lengths = [len(tokens) for tokens in data_train[\"tokens\"]]\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(training_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "65832 words total, with a vocabulary size of 13859\nMax sentence length is 44\n"
     ]
    }
   ],
   "source": [
    "# repeat same thing for the test dataset.\n",
    "\n",
    "all_test_words = [word for tokens in data_test[\"tokens\"] for word in tokens]\n",
    "test_sentence_lengths = [len(tokens) for tokens in data_test[\"tokens\"]]\n",
    "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(test_sentence_lengths))\n"
   ]
  },
  {
   "source": [
    "## Building the RNN\n",
    "\n",
    "https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "import numpy as np \n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newly added Cell\n",
    "dataset = data[['tokens','civic_issue']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = train_test_split(dataset,test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'Text_Final'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-6e1a4de03081>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m45\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\preprocessing\\sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[1;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[0;32m    156\u001b[0m   return sequence.pad_sequences(\n\u001b[0;32m    157\u001b[0m       \u001b[0msequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m       padding=padding, truncating=truncating, value=value)\n\u001b[0m\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m keras_export(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras_preprocessing\\sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[1;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;31m# check `trunc` has expected shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[0mtrunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtrunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m             raise ValueError('Shape of sample %s of sequence at position %s '\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'Text_Final'"
     ]
    }
   ],
   "source": [
    "max_length = 45\n",
    "X_train = sequence.pad_sequences(train, maxlen=max_length)\n",
    "X_test = sequence.pad_sequences(test, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_vecor_length = 32\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(LSTM(100))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# print(model.summary())\n",
    "# model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "# # Final evaluation of the model\n",
    "# scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "# print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}