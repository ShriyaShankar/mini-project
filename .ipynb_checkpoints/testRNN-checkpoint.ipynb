{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/@saad.arshad102/sentiment-analysis-text-classification-using-rnn-bi-lstm-recurrent-neural-network-81086dda8472"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/shriya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32120"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "temp = pd.read_csv('finalData.csv')\n",
    "data = pd.DataFrame()\n",
    "data = temp[['description', 'civic_issue']].copy()\n",
    "data.head()\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>civic_issue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Garbage behind the temple</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Air pollution</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Air pollution in hebbal</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Garbage is dumped near BES</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Leaf and garbage burning on Shakthi Ganapathi ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  civic_issue\n",
       "0                          Garbage behind the temple            1\n",
       "1                                      Air pollution            1\n",
       "2                            Air pollution in hebbal            1\n",
       "3                         Garbage is dumped near BES            1\n",
       "4  Leaf and garbage burning on Shakthi Ganapathi ...            1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of rows, columns\n",
    "data.shape \n",
    "\n",
    "#number of positive, negative (positive -> civic, negative -> non civic)\n",
    "data.civic_issue.value_counts()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                Garbage behind the temple\n",
      "1                                            Air pollution\n",
      "2                                  Air pollution in hebbal\n",
      "3                               Garbage is dumped near BES\n",
      "4        Leaf and garbage burning on Shakthi Ganapathi ...\n",
      "                               ...                        \n",
      "32115    gets new phone hopefully tomorrow but mail doe...\n",
      "32116    LeslieG stack is injured  are you a rowdy fan ...\n",
      "32117                        ryanbader  Bummerlove Chuck  \n",
      "32118         Kinda sad that my show time is over for now \n",
      "32119              good night and its still only day four \n",
      "Name: Text_Clean, Length: 32120, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#data cleaning\n",
    "# 1) we remove punctuation marks\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    text_nopunct = ''\n",
    "    text_nopunct = re.sub('['+string.punctuation+']', '', text)\n",
    "    #text_nopunct = str.maketrans('', '', string.punctuation)\n",
    "    return text_nopunct\n",
    "data['Text_Clean'] = data['description'].apply(lambda x: remove_punctuation(x))\n",
    "\n",
    "print(data['Text_Clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) tokenisation (split the sentences into its respective words)\n",
    "tokens = [(word_tokenize(str(sen))) for sen in data.Text_Clean]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3) convert these tokenised words into their lower cases\n",
    "def lower_token(tokens): \n",
    "    return [w.lower() for w in tokens]    \n",
    "    \n",
    "lower_tokens = [lower_token(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) remove the stopwords using NLTK's stopwords\n",
    "stoplist = stopwords.words('english')\n",
    "def removeStopWords(tokens): \n",
    "    return [word for word in tokens if word not in stoplist]\n",
    "filtered_words = [removeStopWords(sen) for sen in lower_tokens]\n",
    "data['Text_Final'] = [' '.join(sen) for sen in filtered_words]\n",
    "data['tokens'] = filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text_Final</th>\n",
       "      <th>tokens</th>\n",
       "      <th>civic_issue</th>\n",
       "      <th>Civic</th>\n",
       "      <th>Non_Civic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>garbage behind temple</td>\n",
       "      <td>[garbage, behind, temple]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>air pollution</td>\n",
       "      <td>[air, pollution]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>air pollution hebbal</td>\n",
       "      <td>[air, pollution, hebbal]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>garbage dumped near bes</td>\n",
       "      <td>[garbage, dumped, near, bes]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>leaf garbage burning shakthi ganapathi temple ...</td>\n",
       "      <td>[leaf, garbage, burning, shakthi, ganapathi, t...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Text_Final  \\\n",
       "0                              garbage behind temple   \n",
       "1                                      air pollution   \n",
       "2                               air pollution hebbal   \n",
       "3                            garbage dumped near bes   \n",
       "4  leaf garbage burning shakthi ganapathi temple ...   \n",
       "\n",
       "                                              tokens  civic_issue  Civic  \\\n",
       "0                          [garbage, behind, temple]            1      1   \n",
       "1                                   [air, pollution]            1      1   \n",
       "2                           [air, pollution, hebbal]            1      1   \n",
       "3                       [garbage, dumped, near, bes]            1      1   \n",
       "4  [leaf, garbage, burning, shakthi, ganapathi, t...            1      1   \n",
       "\n",
       "   Non_Civic  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since we have a binary category model (civic issue or non civic issue), \n",
    "#we add two one hot encoded columns to our data frame\n",
    "\n",
    "civic = []\n",
    "non_civic = []\n",
    "for l in data.civic_issue:\n",
    "    if l == 0:\n",
    "        civic.append(0)\n",
    "        non_civic.append(1)\n",
    "    elif l == 1:\n",
    "        civic.append(1)\n",
    "        non_civic.append(0)\n",
    "data['Civic']= civic\n",
    "data['Non_Civic']= non_civic\n",
    "data = data[['Text_Final', 'tokens', 'civic_issue', 'Civic', 'Non_Civic']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we split the data into training and testing. i have taken 80-20. \n",
    "# not entirely sure if we should use the 'random_state' parameter.\n",
    "\n",
    "data_train, data_test = train_test_split(data, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263691 words total, with a vocabulary size of 34562\n",
      "Max sentence length is 47\n"
     ]
    }
   ],
   "source": [
    "# we find the total number of words in our training dataset and find max sentence lengeth.\n",
    "\n",
    "all_training_words = [word for tokens in data_train[\"tokens\"] for word in tokens]\n",
    "training_sentence_lengths = [len(tokens) for tokens in data_train[\"tokens\"]]\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(training_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65832 words total, with a vocabulary size of 13859\n",
      "Max sentence length is 44\n"
     ]
    }
   ],
   "source": [
    "# repeat same thing for the test dataset.\n",
    "\n",
    "all_test_words = [word for tokens in data_test[\"tokens\"] for word in tokens]\n",
    "test_sentence_lengths = [len(tokens) for tokens in data_test[\"tokens\"]]\n",
    "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(test_sentence_lengths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
